{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lunar Lander with Reinforcement Learning\n",
    "\n",
    "This notebook demonstrates how to train an agent to land a lunar module on the moon's surface using reinforcement learning. We'll use the Proximal Policy Optimization (PPO) algorithm from the Stable-Baselines3 library.\n",
    "\n",
    "## What is Reinforcement Learning?\n",
    "\n",
    "Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize a reward. The agent learns through trial and error, receiving feedback in the form of rewards or penalties.\n",
    "\n",
    "## The Lunar Lander Environment\n",
    "\n",
    "In this environment, the agent controls a lunar lander with four actions:\n",
    "- Do nothing\n",
    "- Fire left engine\n",
    "- Fire main engine\n",
    "- Fire right engine\n",
    "\n",
    "The goal is to land safely between the flags without crashing. The agent receives:\n",
    "- Positive rewards for moving toward the landing pad and for a safe landing\n",
    "- Negative rewards for crashing or using fuel\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Dependencies\n",
    "\n",
    "First, we need to install the necessary libraries. Uncomment and run the cell below if this is your first time running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Uncomment and run the first time to install the dependencies...\n",
    "#!pip3 install gymnasium stable_baselines3[extra] box2d ipywidgets ffmpeg imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Basic Training Setup\n",
    "\n",
    "In this cell, we:\n",
    "1. Import the necessary libraries\n",
    "2. Create the Lunar Lander environment\n",
    "3. Set up a PPO (Proximal Policy Optimization) agent\n",
    "4. Train the agent for 100,000 timesteps\n",
    "5. Save the trained model\n",
    "\n",
    "PPO is a popular RL algorithm that balances exploration and exploitation while being relatively sample-efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# Create environment\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
    "\n",
    "# Instantiate the agent\n",
    "model = PPO(\n",
    "    'MlpPolicy',\n",
    "    env,\n",
    "    verbose=1,\n",
    "    ##### YOUR HYPERPARAMETERS HERE!!!!\n",
    "    learning_rate=0.001,\n",
    "    batch_size=32,\n",
    "    )\n",
    "\n",
    "# Train the agent and display a progress bar\n",
    "model.learn(total_timesteps=int(100000), progress_bar=True)\n",
    "\n",
    "# Save the agent\n",
    "model.save(\"ppo_lunar\")\n",
    "#del model  # delete trained model to demonstrate loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Advanced Training with Custom Progress Tracking\n",
    "\n",
    "This cell provides a more sophisticated training approach with:\n",
    "\n",
    "1. A custom progress bar callback to track training progress\n",
    "2. TensorBoard logging for visualizing training metrics\n",
    "3. Evaluation of the trained agent's performance\n",
    "\n",
    "The custom callback allows us to see both the progress bar and the training metrics simultaneously, giving us better insight into how the training is progressing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class CustomProgressBarCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Custom callback that combines progress bar with training metrics.\n",
    "    \"\"\"\n",
    "    def __init__(self, total_timesteps):\n",
    "        super().__init__()\n",
    "        self.pbar = None\n",
    "        self.total_timesteps = total_timesteps\n",
    "        self.n_calls = 0\n",
    "\n",
    "    def _on_training_start(self):\n",
    "        self.pbar = tqdm(total=self.total_timesteps)\n",
    "\n",
    "    def _on_step(self):\n",
    "        n_steps = self.locals.get('n_steps', 1)\n",
    "        self.pbar.update(n_steps)\n",
    "        return True\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        self.pbar.close()\n",
    "        self.pbar = None\n",
    "\n",
    "# Create environment\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
    "\n",
    "# Instantiate the agent\n",
    "model = PPO(\n",
    "    'MlpPolicy',\n",
    "    env,\n",
    "    verbose=1,  # Keep verbose=1 to see the training metrics\n",
    "    learning_rate=0.001,\n",
    "    batch_size=32,\n",
    "    tensorboard_log=\"./lunar_lander_tensorboard/\"\n",
    ")\n",
    "\n",
    "# Total timesteps for training\n",
    "total_timesteps = 10000\n",
    "\n",
    "# Create and use the custom callback\n",
    "callback = CustomProgressBarCallback(total_timesteps)\n",
    "\n",
    "# Train the agent\n",
    "model.learn(\n",
    "    total_timesteps=total_timesteps,\n",
    "    callback=callback,\n",
    "    progress_bar=False  # Disable default progress bar to use our custom one\n",
    ")\n",
    "\n",
    "# Save the agent\n",
    "model.save(\"ppo_lunar\")\n",
    "\n",
    "# Evaluate the trained agent\n",
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "print('\\nFinal Evaluation:')\n",
    "print(f'Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Visualizing the Trained Agent\n",
    "\n",
    "Now that we have a trained agent, let's see how it performs! This cell:\n",
    "\n",
    "1. Loads the trained model\n",
    "2. Runs the agent in the environment for 5 episodes\n",
    "3. Captures frames from each episode\n",
    "4. Saves the episodes as GIF animations\n",
    "\n",
    "This visualization helps us understand how well our agent has learned to land the lunar module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Create images directory if it doesn't exist\n",
    "if not os.path.exists(\"images\"):\n",
    "    os.makedirs(\"images\")\n",
    "\n",
    "# Load the trained agent\n",
    "env = gym.make(\"LunarLander-v3\", render_mode=\"rgb_array\")\n",
    "model = PPO.load(\"ppo_lunar\", env=env)\n",
    "\n",
    "# Evaluate the agent\n",
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "print('Mean reward:', mean_reward, 'Std. reward:', std_reward)\n",
    "\n",
    "# Test the trained agent and save visualization\n",
    "images = []\n",
    "episodes = 0\n",
    "obs, _ = env.reset()  # Updated reset call syntax\n",
    "\n",
    "while episodes < 5:  # Limit to 5 episodes for reasonable file sizes\n",
    "    img = env.render()\n",
    "    images.append(img)\n",
    "    \n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, terminated, truncated, info = env.step(action)  # Updated step call syntax\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        episodes += 1\n",
    "        print(f'Episode {episodes} finished with reward {reward}')\n",
    "        \n",
    "        # Save episode as GIF\n",
    "        if len(images) > 0:\n",
    "            print(f'Saving episode {episodes} animation...')\n",
    "            imageio.mimsave(\n",
    "                f'images/lunar_lander_episode_{episodes}.gif',\n",
    "                images,\n",
    "                fps=30\n",
    "            )\n",
    "        \n",
    "        # Reset for next episode\n",
    "        images = []\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "env.close()\n",
    "print(\"Done! Check the 'images' directory for the animation files.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Displaying the Results\n",
    "\n",
    "Finally, let's display the GIF animations we created to see our agent in action. This cell:\n",
    "\n",
    "1. Checks if the images directory exists\n",
    "2. Finds all the GIF files we created\n",
    "3. Displays each animation in the notebook\n",
    "\n",
    "This gives us a visual representation of how our agent performs after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and show GIF animations from the images directory\n",
    "\n",
    "from IPython.display import Image, display\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Check if images directory exists and contains GIF files\n",
    "if not os.path.exists('images'):\n",
    "    print('Error: images directory not found. Please run the previous cell to generate the GIF files.')\n",
    "else:\n",
    "    gif_files = glob.glob('images/lunar_lander_episode_*.gif')\n",
    "    if not gif_files:\n",
    "        print('Error: No GIF files found in the images directory. Please run the previous cell to generate the GIF files.')\n",
    "    else:\n",
    "        print(f'Found {len(gif_files)} GIF files in the images directory.')\n",
    "        \n",
    "        # Display last GIF\n",
    "        for i, gif_file in enumerate(sorted(gif_files), 1):\n",
    "            print(f'\\nEpisode {i}:')\n",
    "            display(Image(filename=gif_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You've successfully:\n",
    "\n",
    "1. Set up a reinforcement learning environment (Lunar Lander)\n",
    "2. Trained an agent using the PPO algorithm\n",
    "3. Evaluated the agent's performance\n",
    "4. Visualized the agent's behavior\n",
    "\n",
    "### Key Concepts Learned:\n",
    "\n",
    "- **States**: The position, velocity, and orientation of the lunar lander\n",
    "- **Actions**: The four possible controls (do nothing, fire left/right/main engines)\n",
    "- **Rewards**: Feedback based on landing position, fuel usage, and crash/success\n",
    "- **Policy**: The strategy the agent learns to maximize rewards\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Try modifying the hyperparameters to see if you can improve performance\n",
    "- Experiment with different RL algorithms (like A2C, DQN, or SAC)\n",
    "- Increase the training time to see if the agent can achieve better results\n",
    "- Try more complex environments from the Gymnasium library"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
