{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent Soccer with ML-Agents\n",
    "\n",
    "In this notebook, we'll train a 2vs2 soccer team using Unity ML-Agents. We'll use the MA-POCA (Multi-Agent POsthumous Credit Assignment) algorithm to train cooperative behavior among agents.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The environment is called `SoccerTwos`. The goal is to get the ball into the opponent's goal while preventing the ball from entering your own goal.\n",
    "\n",
    "This notebook will guide you through:\n",
    "1. Setting up ML-Agents\n",
    "2. Understanding the environment\n",
    "3. Understanding MA-POCA algorithm\n",
    "4. Configuring training parameters\n",
    "5. Training the agents\n",
    "6. Pushing the trained model to Hugging Face Hub\n",
    "7. Participating in AI vs. AI challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Install ML-Agents and Download the Environment\n",
    "\n",
    "First, we need to install ML-Agents and download the SoccerTwos environment executable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ML-Agents\n",
    "!git clone https://github.com/Unity-Technologies/ml-agents\n",
    "%cd ml-agents\n",
    "!pip install -e ./ml-agents-envs\n",
    "!pip install -e ./ml-agents\n",
    "\n",
    "# For Mac users on Apple Silicon who encounter issues\n",
    "# !conda install grpcio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Environment Executable\n",
    "\n",
    "Based on your operating system, download one of these executables, unzip it, and place it in a new folder inside `ml-agents` called `training-envs-executables`:\n",
    "\n",
    "- Windows: [Download executable](https://drive.google.com/file/d/1sqFxbEdTMubjVktnV4C6ICjp89wLhUcP/view?usp=sharing)\n",
    "- Linux (Ubuntu): [Download executable](https://drive.google.com/file/d/1KuqBKYiXiIcU4kNMqEzhgypuFP5_45CL/view?usp=sharing)\n",
    "- Mac: [Download executable](https://drive.google.com/drive/folders/1h7YB0qwjoxxghApQdEUQmk95ZwIDxrPG?usp=share_link)\n",
    "\n",
    "For Mac users, you'll also need to run this command to make the executable runnable:\n",
    "```bash\n",
    "xattr -cr training-envs-executables/SoccerTwos/SoccerTwos.app\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Understand the Environment\n",
    "\n",
    "### SoccerTwos Environment\n",
    "\n",
    "The SoccerTwos environment is a 2vs2 soccer game where each team has two agents. The goal is to get the ball into the opponent's goal while preventing the ball from entering your own goal.\n",
    "\n",
    "### Reward Function\n",
    "\n",
    "The reward function is:\n",
    "- +1 when the team scores a goal\n",
    "- -1 when the opponent team scores a goal\n",
    "- Small negative reward each step to encourage faster goal-scoring\n",
    "\n",
    "### Observation Space\n",
    "\n",
    "The observation space is composed of vectors of size 336:\n",
    "- 11 ray-casts forward distributed over 120 degrees (264 state dimensions)\n",
    "- 3 ray-casts backward distributed over 90 degrees (72 state dimensions)\n",
    "- Both of these ray-casts can detect 6 objects:\n",
    "  - Ball\n",
    "  - Blue Goal\n",
    "  - Purple Goal\n",
    "  - Wall\n",
    "  - Blue Agent\n",
    "  - Purple Agent\n",
    "\n",
    "### Action Space\n",
    "\n",
    "The action space consists of three discrete branches:\n",
    "1. Forward/Backward movement\n",
    "2. Rotation\n",
    "3. Side movement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Understand MA-POCA\n",
    "\n",
    "MA-POCA (Multi-Agent POsthumous Credit Assignment) is a training algorithm designed for cooperative multi-agent scenarios.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **Centralized Critic**: A centralized critic processes the states of all agents in the team to estimate how well each agent is doing. Think of this critic as a coach.\n",
    "\n",
    "2. **Decentralized Execution**: Each agent makes decisions based only on what it perceives locally.\n",
    "\n",
    "3. **Credit Assignment**: The algorithm helps determine the contribution of each agent to the team's success.\n",
    "\n",
    "This approach combines Self-Play (to learn competitive behavior against opponents) with MA-POCA (to learn cooperative behavior within the team)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define the Configuration File\n",
    "\n",
    "Let's create a configuration file for our training. This file will define the hyperparameters for the MA-POCA algorithm and self-play settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config/poca/SoccerTwos.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config/poca/SoccerTwos.yaml\n",
    "behaviors:\n",
    "  SoccerTwos:\n",
    "    trainer_type: poca\n",
    "    hyperparameters:\n",
    "      batch_size: 2048\n",
    "      buffer_size: 20480\n",
    "      learning_rate: 0.0003\n",
    "      beta: 0.005\n",
    "      epsilon: 0.2\n",
    "      lambd: 0.95\n",
    "      num_epoch: 3\n",
    "      learning_rate_schedule: constant\n",
    "    network_settings:\n",
    "      normalize: false\n",
    "      hidden_units: 512\n",
    "      num_layers: 2\n",
    "      vis_encode_type: simple\n",
    "    reward_signals:\n",
    "      extrinsic:\n",
    "        gamma: 0.99\n",
    "        strength: 1.0\n",
    "    keep_checkpoints: 5\n",
    "    max_steps: 5000000\n",
    "    time_horizon: 1000\n",
    "    summary_freq: 10000\n",
    "    self_play:\n",
    "      save_steps: 50000\n",
    "      team_change: 200000\n",
    "      swap_steps: 2000\n",
    "      window: 10\n",
    "      play_against_latest_model_ratio: 0.5\n",
    "      initial_elo: 1200.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Configuration Parameters\n",
    "\n",
    "Let's break down some of the key parameters:\n",
    "\n",
    "#### Hyperparameters:\n",
    "- `batch_size`: Number of experiences in each iteration of gradient descent\n",
    "- `buffer_size`: Number of experiences to collect before updating the policy\n",
    "- `learning_rate`: Strength of each gradient descent update step\n",
    "- `epsilon`: Acceptable range for policy deviation during training\n",
    "\n",
    "#### Network Settings:\n",
    "- `hidden_units`: Number of units in the hidden layers\n",
    "- `num_layers`: Number of hidden layers\n",
    "\n",
    "#### Self-Play Parameters:\n",
    "- `save_steps`: Number of steps between snapshots of the policy\n",
    "- `team_change`: Number of steps between swapping teams\n",
    "- `swap_steps`: Number of steps between changing the opponent policy\n",
    "- `window`: Number of past policies to choose from for the opponent\n",
    "- `play_against_latest_model_ratio`: Probability of playing against the most recent policy\n",
    "\n",
    "You can modify these parameters to experiment with different training strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Start the Training\n",
    "\n",
    "Now we're ready to start training our agents. This will take several hours (5-8 hours for 5M timesteps), so be prepared to let your computer run for a while.\n",
    "\n",
    "**Note**: The training command below should be run in a terminal, not in this notebook, as it will run for many hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: mlagents-learn: command not found\n"
     ]
    }
   ],
   "source": [
    "# This is the command you should run in a terminal\n",
    "# For Windows:\n",
    "# !mlagents-learn ./config/poca/SoccerTwos.yaml --env=./training-envs-executables/SoccerTwos.exe --run-id=\"SoccerTwos\" --no-graphics\n",
    "\n",
    "# For Mac:\n",
    "!mlagents-learn ./config/poca/SoccerTwos.yaml --env=./training-envs-executables/SoccerTwos/SoccerTwos.app --run-id=\"SoccerTwos\" --no-graphics\n",
    "\n",
    "# For Linux:\n",
    "# !mlagents-learn ./config/poca/SoccerTwos.yaml --env=./training-envs-executables/SoccerTwos/SoccerTwos.x86_64 --run-id=\"SoccerTwos\" --no-graphics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Tips\n",
    "\n",
    "- It's normal if you don't see a big increase in ELO score (or even see a decrease below 1200) before 2M timesteps. Your agents will spend most of their time moving randomly on the field before being able to score goals.\n",
    "- You can stop the training with Ctrl + C, but be careful to only press it once to allow ML-Agents to generate the final .onnx file before closing.\n",
    "- Training progress can be monitored using TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Push the Agent to the Hugging Face Hub\n",
    "\n",
    "After training, you can push your model to the Hugging Face Hub to participate in the AI vs. AI challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to Hugging Face\n",
    "!huggingface-cli login\n",
    "\n",
    "# Push your model to the Hub\n",
    "# Replace the placeholders with your actual values\n",
    "# !mlagents-push-to-hf --run-id=\"SoccerTwos\" --local-dir=\"./results/SoccerTwos\" --repo-id=\"YourUsername/poca-SoccerTwos\" --commit-message=\"First Push\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Verify Your Model for the AI vs. AI Challenge\n",
    "\n",
    "To ensure your model is ready for the AI vs. AI Challenge, check that:\n",
    "\n",
    "1. Your model has the tag `ML-Agents-SoccerTwos` in its Hugging Face repository\n",
    "2. Your repository contains a `SoccerTwos.onnx` file\n",
    "\n",
    "If these conditions are met, your model will be automatically added to the challenge pool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Visualize Matches in the Demo\n",
    "\n",
    "You can visualize how your model performs against others using the [ML-Agents-SoccerTwos demo](https://huggingface.co/spaces/unity/ML-Agents-SoccerTwos).\n",
    "\n",
    "To do this:\n",
    "1. Go to the demo\n",
    "2. Select your model as team blue (or purple)\n",
    "3. Select another model to compete against\n",
    "4. Watch the match!\n",
    "\n",
    "The best opponents to compare your model against are either the top models on the leaderboard or the [baseline model](https://huggingface.co/unity/MLAgents-SoccerTwos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've learned how to:\n",
    "- Set up ML-Agents for multi-agent reinforcement learning\n",
    "- Understand the SoccerTwos environment and MA-POCA algorithm\n",
    "- Configure and train cooperative agents\n",
    "- Push models to the Hugging Face Hub\n",
    "- Participate in AI vs. AI challenges\n",
    "\n",
    "This represents a practical application of multi-agent reinforcement learning where agents need to both cooperate with teammates and compete against opponents."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
